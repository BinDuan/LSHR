---
title: "Locality Sensitive Hashing in R. Overview of LSHR package."
author: "Dmitriy Selivanov"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Locality Sensitive Hashing in R. Overview of LSHR package.}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
# Locality Sensitive Hashing in R
LSHR - fast and memory efficient package for near-neighbor search in high-dimensional data. Only minhashing implemented at the moment.
Most of ideas are based on brilliant [Mining of Massive Datasets](http://www.mmds.org) book - many thanks to authors. 

# Contents
In this vignette I will try to explain base concepts of [Locality Sensitive Hashing](http://en.wikipedia.org/wiki/Locality-sensitive_hashing) and [Minhash](http://en.wikipedia.org/wiki/MinHash) techniques. Also we will touch some details about implementation of these algorithms in **LSHR** package.

1. [Quick reference](#quick-reference)
2. [Overview of the problem](#the-problem)
3. [Minhashing](#minhashing)
4. [Implementation details](#implementation)

# Quick reference
```{r quick-ref}
set.seed(10)
library(LSHR)
# generate element of sets
elems <- sapply(1:100, function(z) 
  paste(sample(letters, sample(3:7), replace = T), collapse=''))
# generate sets
sets <-  lapply(1:100, function(z) sample(elems, sample(10:40)))
# add near-duplicates
sets <- c(sets, lapply(sets[1:10], function(x) c(x, sample(elems, 5))  ))
m <- get_signature_matrix(sets = sets, hashfun_number = 60, cores = 2)
candidate_indices <- get_candidate_pairs(signature_matrix = m, 
                                         bands_number = 10, similarity = 0.8, verbose = T)
candidate_indices
```

# The problem
Imagine the following interesting problems.

- We are developing a service similar to google news. And we want to **clusterize** similar news into groups so each group will represent cluster of articles about particular event. Of course there are a lot of methods for comapring text. One common is [cosine similarity](http://en.wikipedia.org/wiki/Cosine_similarity). But if we have millions of documents it is hard to compute this distance between each pair because of quadratic complexity. And it is hard to store such big similarity matrix.
- We have two *very large* social netwotks (for example *facebook and google+*), which have hundreds of millions of profiles. And we want to determine profiles owned by same person - **find near-duplicates**. One reasonable approach is to assume that people who have particular firstname, lastname and nearly same, or at least highly overlapped, sets of friends in both networks are good candidates for detailed check. 
- you can find more applications of LSH [here](http://en.wikipedia.org/wiki/Locality-sensitive_hashing#Applications) at Wikipedia.

One well known measure for determining degree of similarity of sets is [Jaccard Index](http://en.wikipedia.org/wiki/Jaccard_index) or Jaccard similarity:  
$$J(SET_1, SET_2) = {|SET_1 \cap SET_2|\over |SET_1 \cup SET_2| }$$

# Overview of minhashing technique

## Possible brute-force solution 
Let try to focus on [second problem above](#the-problem). At first look it seems that we easily can compute similarity of all pairs, because `union` and `intersection` set operations are quite cheap and fast.  
Suppose we have `5000` *John Smith* profiles from *facebook* and `5000` *John Smith* profiles from *google+* - `10000` total. So we have to calclulate `5000 * 5000` (25 millions!) of jaccard distances to find good candidates.  
First of all let's generate artificial set of lastnames - we will pick "friends lastnames" for our candidates from this set.
```{r}
# for reproducible results
set.seed(seed = 17)
# we like %>% pipe operator! =)
library('magrittr')
# generate about 10000 "lastnames"
lastnames <- sapply(1:1e4, function(x) 
  sample(letters, sample(4:8)) %>% paste(collapse = '')) %>% 
  unique
print(head(lastnames))
```
Pick first pair of candidates, generate friends sets.
```{r}
candidate_1_friends_set <- sample(lastnames, sample(20:150), replace = F)
candidate_2_friends_set <- sample(lastnames, sample(20:150), replace = F)
```
Now we will try to evaluate computation time for our brure-force approach.
Jaccard similarity:
```{r}
jaccard <- function(x, y) {
  set_intersection <- length(intersect(x, y))
  set_union <- length(union(x, y))
  return(set_intersection / set_union)
}
```
Naive benchmark:
```{r}
library(microbenchmark)
timings <- microbenchmark(jaccard(candidate_1_friends_set, candidate_2_friends_set))
print(timings)
mean_timings <- mean(timings[['time']])
# convert from nano-seconds to seconds
mean_timings <- mean_timings * 1e-9
```
On my computer one `jaccard()` call takes about 25 microseconds. So entire calculation will cost about 10 hours on my computer:
```{r}
mean_timings * 5000 * 5000 / 60
```
May be this simple `jaccard()` implementation is not very effective and we can reduce computational time by a factor of 10-20 (which is quite reasonable - try to implement `jaccard()` function in pure C/C++). But even 1 hour is quite much for such toy example. Real-world datasets are much much larger.
Then main problem is in $O(n^2)$ complexity of our brute-force algorithm.  

## Minhashing
To solve this kind problem we will use [Locality-sensitive hashing]((http://en.wikipedia.org/wiki/Locality-sensitive_hashing)) - a method of performing probabilistic dimension reduction of high-dimensional data. It provides good tradeoff between accuracy and computational time and roughly speaking has $O(n)$ complexity.  
I this vignette we will focus on one scheme of **LSH**, called [MinHashing](http://en.wikipedia.org/wiki/MinHash).  
The intuition of the method is the following: we will try to hash the input items so that similar items are mapped to the same buckets with high probability (the number of buckets being much smaller than the universe of possible input items).  
Let's construct simple example:
```{r toy-example-generate}
set1 <- c('SMITH', 'JOHNSON', 'WILLIAMS', 'BROWN')
set2 <- c('SMITH', 'JOHNSON', 'BROWN')
set3 <- c('THOMAS', 'MARTINEZ', 'DAVIS')
set_list <- list(set1, set2, set3)
```
Now we have 3 sets to compare and identify profiles, related to same "John Smith". From these sets we will construct matrix which encode relations between sets:
```{r toy-example-matrix-conctruction-1}
sets_dict <- unlist(set_list) %>% unique
m <- sapply(set_list, 
            function(set, dict) as.integer(dict %in% set), 
            dict = sets_dict, 
            simplify = T)
dimnames(m) <- list(sets_dict, paste('set', 1:length(set_list), sep = '_'))
```
```{r, echo=FALSE, results='asis'}
knitr::kable(m)
```

Note, that this matrix is very similar to term-document matrix used in text-mining applications.
Let's call it matrix **input-matrix** $M$.
In our representation similarity of two sets from source array equal to the similarity of two corresponding columns with non-zero rows:  

name | set_1 | set_2 | intersecton | union|
---|---|---|---|---|
SMITH|1|1|+|+|
JOHNSON|1|1|+|+|
WILLIAMS|1|0|-|+|
BROWN|1|1|+|+|
THOMAS|0|0|-|-|
MARTINEZ|0|0|-|-|
DAVIS|0|0|-|-|

From table above we can conclude, that **jaccard index between set\_1 and set\_2 is 0.75**.  
Let's check:
```{r toy-example-matrix-similarity}
print(jaccard(set1, set2))
```
Prove that jaccard index between non-zero rows is equal to jaccard index between input sets:
```{r}
column_jaccard <-  function(c1, c2) {
  non_zero <- which(c1 | c2)
  column_intersect <- sum(c1[non_zero] & c2[non_zero])
  column_union <- length(non_zero)
  return(column_intersect / column_union)
}
isTRUE(jaccard(set1, set2) == column_jaccard(m[, 1], m[, 2]))
```

### Minhashing algorithm
1. Pick column $c$ from **input-matrix** $M$.
1. Define **minhash function** $h(c)$ = # of first row in which column $c == 1$.
1. Now suppose random permutation of rows of the $M$. If we will use $N$ **independent** permutations we will end with $N$ **minhash functions**. 
So now we can construct **signature-matrix** from $M$ using these minhash functions:
```{r toy-example-minhash-v1}
# for our toy example we will pick N = 4
N <- 4
sm <- matrix(data = NA_integer_, nrow = N, ncol = ncol(m))
perms <- matrix(data = NA_integer_, nrow = nrow(m), ncol = N)
# calculate indexes for non-zero entries for each column
non_zero_row_indexes <- apply(m, MARGIN = 2, FUN = function(x) which (x != 0) )
for (i in 1 : N) {
  # calculate permutations
  perm <- sample(nrow(m))
  perms[, i] <- perm
  # fill row of signature matrix
  for (j in 1:ncol(m))
    sm[i, j] <-  min(perm[non_zero_row_indexes[[j]]])
}
print(sm)
```
In chunk above we did it not very efficiently with 2 nested ```for``` loops. But the logic should be very clear.

You can see how we obtain **signature-matrix** matrix after "minhash transformation". Permutations and corresponding signatures marked with same colors:

|perm_1|perm_2|perm_3|perm_4|set_1| set_2| set_3|
 |---|---|---|---|---|---|---|
 <span style="background-color:lightgreen">4 </span>| <span style="background-color:orange">1 </span>| <span style="background-color:lightblue">4 </span>| <span style="background-color:yellow">6 </span>| 1 | 1 | 0 |
 <span style="background-color:lightgreen">3 </span>| <span style="background-color:orange">4 </span>| <span style="background-color:lightblue">1 </span>| <span style="background-color:yellow">1 </span>| 1 | 1 | 0 |
 <span style="background-color:lightgreen">7 </span>| <span style="background-color:orange">6 </span>| <span style="background-color:lightblue">6 </span>| <span style="background-color:yellow">2 </span>| 1 | 0 | 0 |
 <span style="background-color:lightgreen">6 </span>| <span style="background-color:orange">2 </span>| <span style="background-color:lightblue">7 </span>| <span style="background-color:yellow">3 </span>| 1 | 1 | 0 |
 <span style="background-color:lightgreen">5 </span>| <span style="background-color:orange">3 </span>| <span style="background-color:lightblue">2 </span>| <span style="background-color:yellow">5 </span>| 0 | 0 | 1 |
 <span style="background-color:lightgreen">2 </span>| <span style="background-color:orange">5 </span>| <span style="background-color:lightblue">3 </span>| <span style="background-color:yellow">7 </span>| 0 | 0 | 1 |
 <span style="background-color:lightgreen">1 </span>| <span style="background-color:orange">7 </span>| <span style="background-color:lightblue">5 </span>| <span style="background-color:yellow">4 </span>| 0 | 0 | 1 |


|set_1| set_2| set_3|
|---|---|---|
|<span style="background-color:lightgreen">3</span>|<span style="background-color:lightgreen">3</span>|<span style="background-color:lightgreen">1</span>|
|<span style="background-color:orange">1</span>|<span style="background-color:orange">1</span>|<span style="background-color:orange">3</span>|
|<span style="background-color:lightblue">1</span>|<span style="background-color:lightblue">1</span>|<span style="background-color:lightblue">2</span>|
|<span style="background-color:yellow">1</span>|<span style="background-color:yellow">1</span>|<span style="background-color:yellow">4</span>|

Notice that $set_1$ and $set_2$ signatures are very similar and signature of $set_3$ dissimilar with $set_1$ and $set_2$.
```{r toy-example-signatures-sim}
jaccard_signatures <-  function(c1, c2) {
  column_intersect <- sum(c1 == c2)
  column_union <- length(c1)
  return(column_intersect / column_union)
}
print(jaccard_signatures(sm[, 1], sm[, 2]))
print(jaccard_signatures(sm[, 1], sm[, 3]))
```
### Intuition and theoretical guaranties

Intuition is very straighforward. Let's look down the permuted columns $c_1$ and $c_2$ until we detect **1**.  

  * If in both columns we find ones - (1, 1), then $h(c_1) = h(c_2)$. 
  * In case (0, 1) or (1, 0) $h(c_1) \neq h(c_2)$. 
  
So the probability over all permutations of non-zero rows that $h(c_1) = h(c_2)$ is the same as $J(c_1, c_2)$.  
Or we can repharase this as following:

> As from Probability that the minhash function for a random permutation of rows produces the same value for two sets equals the Jaccard similarity of those sets.

Moreover there exist **theoretical guaranties** for estimation of Jaccard similarity: for any constant $\varepsilon > 0$ there is a constant $k = O(1/\varepsilon^2)$ - number of permutations - such that the expected error of the estimate is at most $\varepsilon$. 

## Specific form of Locality Sensitive Hashing for minhashing
After minhashing we obtain quite compact dense signature matrix. Jaccard distance between columns corresponds to distance between input sets. We have control on accuracy - more permutations we will make - more accurate estimation we will recieve. That's cool, but we **still have $O(n^2)$ complexity** - we have to compare all possible pairs to find similat sets.  
That's where LSH come in.
Very good explanation from [MMDS book](mmds.org):

> One general approach to LSH is to *hash* items several times, in such a way that **similar items are more likely to be hashed to the same bucket than dissimilar items are**. We then consider any pair that hashed to the same bucket for any of the hashings to be a **candidate pair**. We check only the candidate pairs for similarity. The hope is that most of the dissimilar pairs will never hash to the same bucket, and therefore will never be checked. Those dissimilar pairs that do hash to the same bucket are **false positives**; we hope these will be only a small fraction of all pairs. We also hope that most of the truly similar pairs will hash to the same bucket under at least one of the hash functions. Those that do not are **false negatives**; we hope these will be only a small fraction of the truly similar pairs.
If we have minhash signatures for the items, an effective way to choose the hashings is to divide the signature matrix into `b` **bands** consisting of `r` **rows** each. For each band, there is a hash function that takes vectors of `r` integers (the portion of one column within that band) and *hashes them to some large number of buckets*. We can use the same hash function for all the bands, but we use a separate bucket array for each band, so columns with the same vector in different bands will not hash to the same bucket.

Lets look at out signtatures matrix from [example above](#minhashing-algorithm):

|set_1| set_2| set_3|
|---|---|---|
3	| 3	| 1
1	| 1	| 3
1	| 1	| 2
1	| 1	| 4

Now we will make interesting trick - split rows of our matrix into **"bands"**. For example we will split it into 2 bands:  
  
$Band_1$ consists of two first rows:

|set_1| set_2| set_3|
|---|---|---|
3	| 3	| 1
1	| 1	| 3

and $Band_2$ consists of next two rows:

|set_1| set_2| set_3|
|---|---|---|
1	| 1	| 2 |
1	| 1	| 4 |

After that we will **hash each column $c$ of each band**. For example, suppose 
$$hash(c) = concatenation(c)$$ 

So hashes for bands will look like:

$HashBand_1$:

|set_1| set_2| set_3|
|---|---|---|
31	| 31	| 13 |

$HashBand_2$:

|set_1| set_2| set_3|
|---|---|---|
11	| 11	| 24 |

We see, that $set_1$ and $set_2$ have same hash values - they were **hashed into same buckets**, so became **candidate pairs**.  
**Intuition** is following: 

> The more similar two columns are, the more likely it is that they will be identical in some band. Thus, intuitively the banding strategy makes similar columns much more likely to be candidate pairs than dissimilar pairs.

### Formal prove and theoretical guaranties
Suppose we use $b$ bands of $r$ rows each, and suppose that a particular pair of documents have Jaccard similarity $s$. Recall that the probability the minhash signatures for these documents agree in any one particular
row of the signature matrix is $s$. We can calculate the probability that these documents (or rather their signatures) become a candidate pair as follows:

1. The probability that the signatures agree in all rows of one particular
band is $s^r$.
2. The probability that the signatures do not agree in at least one row of a
particular band is $(1 − s)^r$ .
3. The probability that the signatures do not agree in all rows of any of the
bands is $(1 − s^r)^b$ .
4. The probability that the signatures agree in all the rows of at least one
band, and therefore become a candidate pair, is $1 − (1 − s^r)^b$.

# Implementation
**LSHR** package designed to be **fast and memory efficient**. We use [Rcpp](https://github.com/RcppCore/Rcpp) (fast integer hash functions + concurrency with openMP), [fastmatch](https://github.com/s-u/fastmatch) and [data.table](github.com/Rdatatable/data.table/) packages.

There few challenges in developing efficient implementation of minhashing:

1. Construction of **input-matrix** $M$.
  * storage size and computational efficiency
2. Signature matrix construction
  * permutations are expensive! using many hash functions.
  * construction of hash function family
  * vectorized implementation
3. Locality Sensitive Hashing
  * number of buckets
  * theoretical guaranties

## Construction of input term-document matrix
**TODO**

## Signature matrix construction
### Using many hash functions instead of permutations
Suppose **input-matrix** is very big, say ```1e9``` rows. It is quite hard computationally to permute 1 billion rows. Also you need to store these entries and access these values. It is common to use following scheme instead: 

  * Pick $N$ independent hash functions $h_i(c)$ instead of $N$ premutations, $i = 1..N$.  
  * For each column $c$ and each hash function $h_i$, keep a "slot" $M(i, c)$.  
  * $M(i, c)$ will become the smallest value of $h_i(r)$ for which column $c$ has 1 in row $r$. I.e., $h_i(r)$ gives order of rows for $i^{th}$ permutation.  

So we end up with following from excellent [Mining of Massive Datasets](http://www.mmds.org) book:
```
for each row r do begin
  for each hash function hi do
    compute hi (r);
  for each column c
    if c has 1 in row r
      for each hash function hi do
        if hi(r) is smaller than M(i, c) then
          M(i, c) := hi(r);
end;
```
Nested loops are slow in R. We will implement this algoritm in fast [vectorized way](#vectorized-implementation-of-minhashing-algorithm).

### construction of hash function family

### vectorized implementation of minhashing algorithm
